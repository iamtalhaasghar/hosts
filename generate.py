# a script to generate hosts from redlib, libreddit & invidious instance pages
# Talha Asghar <talhaasghar220@gmail.com>
# 02-aug-2024

import re,io
from redis import Redis
from datetime import datetime
import requests
from bs4 import BeautifulSoup

rdb = Redis(decode_responses=True)


prefix = f'''
# this file was autogenerated 
# on {str(datetime.now())} 
# by https://github.com/iamtalhaasghar/hosts/blob/master/generate.py
# total hosts: $total_hosts

127.0.0.1 localhost
127.0.0.1 localhost.localdomain
127.0.0.1 local
255.255.255.255 broadcasthost
::1 localhost
::1 ip6-localhost
::1 ip6-loopback
fe80::1%lo0 localhost
ff00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
0.0.0.0 0.0.0.0
'''


def fetch_reddit_instances():
    hosts = ['https://raw.githubusercontent.com/redlib-org/redlib-instances/main/instances.json', 'https://raw.githubusercontent.com/libreddit/libreddit-instances/master/instances.json']
    for host in hosts:        
        urls = [i['url'].replace('https://', '') for i in requests.get(host).json()['instances'] if 'url' in i.keys()]
        k = '/blacklist/host/reddit'
        for u in urls:
            if not rdb.sismember(k, u):
                print(f'found new reddit instance {u} from {host}')
                rdb.sadd(k, u)

    
def fetch_invidious_instances():
    
    parser1 = lambda host: [i[0] for i in requests.get(host).json()]
    parser2 = lambda host: [i.text for i in BeautifulSoup(requests.get(host).text, 'lxml').find_all('span', class_='alias')]
    parser3 = lambda host: [re.search(r'\[(.*?)\]', i).group(1) for i in requests.get(host).text.split('\n') if i.startswith('*') and 'https' in i]

    hosts = {'https://api.invidious.io/instances.json': parser1, 'https://uptime.invidious.io' : parser2, 'https://raw.githubusercontent.com/iv-org/documentation/master/docs/instances.md': parser3}
    k = '/blacklist/host/invidious'
    for host, parser in hosts.items():
        urls = parser(host)
        #print(host, urls);continue
        for u in urls:        
            if not rdb.sismember(k, u):
                print(f'found new invidious instance {u} from {host}')
                rdb.sadd(k, u)


fetch_invidious_instances()
fetch_reddit_instances()

count = 0 
with io.StringIO() as s:
    s.write(prefix)
    for k in sorted(rdb.keys('/blacklist/host*')):
        c = rdb.scard(k)
        count += c
        s.write(f'\n# {k} - {c} entries\n\n')
        for m in sorted(rdb.smembers(k)):
            if m.endswith('.onion') or m.endswith('.i2p'):
                continue
            s.write(f'0.0.0.0 {m}\n')

    with open('hosts.txt', 'w') as f:
        f.write(s.getvalue().replace('$total_hosts', str(count)))
